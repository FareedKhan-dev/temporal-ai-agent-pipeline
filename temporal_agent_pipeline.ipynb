{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown-1",
   "metadata": {},
   "source": [
    "# Temporal AI Agent for a Dynamic Knowledge Base\n",
    "\n",
    "Modern RAG or Agentic architectures that answer questions depend on a dynamic knowledge base that keeps updating over time, such as financial reports or documentation, so that the reasoning and planning process remains logical and accurate.\n",
    "\n",
    "To handle such a knowledge base, where the size continuously grows and the chances of hallucinations can increase, **a separate logical-temporal (time-aware) agentic pipeline is required to manage this evolving knowledge base** within your AI product. This pipeline includes:\n",
    "\n",
    "![Temporal AI Agent Pipeline](https://miro.medium.com/v2/resize:fit:4800/1*0LeKh5au7MzYlXTKSlSImA.png)\n",
    "\n",
    "1.  **Semantic Chunking:** Breaks down large, raw documents into small, contextually meaningful text chunks.\n",
    "2.  **Atomic Facts:** Uses an LLM to read each chunk and extract atomic facts, their timestamps, and the entities involved.\n",
    "3.  **Entity Resolution:** Cleans the data by automatically finding and merging duplicate entities (e.g., “AMD” and “Advanced Micro Devices”).\n",
    "4.  **Temporal Invalidation:** Intelligently identifies and resolves contradictions by marking outdated facts as “expired” when new information arrives.\n",
    "5.  **Knowledge Graph Construction:** Assembles the final, clean, time-stamped facts into a connected graph structure that our AI agent can query.\n",
    "6.  **Optimized Knowledge Base:** Stores the final, dynamic knowledge graph in a scalable cloud database, creating the reliable, up-to-date “brain” on top of which the final RAG or agentic system is built.\n",
    "\n",
    "In this notebook, we are going to create:\n",
    "\n",
    "> An end-to-end temporal agentic pipeline that transforms raw data into a dynamic knowledge base, and then build a multi-agent system on top of it to measure its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-intro",
   "metadata": {},
   "source": [
    "## Pre-processing and Analyzing our Dynamic Data\n",
    "We will be working with datasets that evolve continuously over time and the financial situation of a company is one of the best examples.\n",
    "\n",
    "![Pre-processing Step](https://miro.medium.com/v2/resize:fit:2000/1*jU-neAC6n7kgl2MnBN7vuw.png)\n",
    "\n",
    "Companies regularly share updates on their financial performance, such as stock price movements, major developments like changes in executive leadership, and forward-looking expectations such as whether quarterly revenue is projected to grow by 12% year-over-year, and so on.\n",
    "\n",
    "To mimic a real world scenario, We will be working with [earnings_call huggingface dataset](https://huggingface.co/datasets/jlh-ibm/earnings_call/viewer/transcripts) from [John Henning](https://huggingface.co/jlh-ibm). It contains information about the financial performance of different companies over a time frame.\n",
    "\n",
    "Let’s load this dataset and perform some statistical analysis on it to get used to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary loader from the langchain_community package\n",
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "hf_dataset_name = \"jlh-ibm/earnings_call\"\n",
    "# The 'name' parameter in the loader corresponds to the subset\n",
    "subset_name = \"transcripts\"\n",
    "\n",
    "# Instantiate the loader with the path and name of the dataset\n",
    "# By default, it loads the 'train' split, which is what the notebook does.\n",
    "loader = HuggingFaceDatasetLoader(path=hf_dataset_name, name=subset_name, page_content_column=\"transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the key step. The loader processes the dataset and returns a list of LangChain Document objects.\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-1",
   "metadata": {},
   "source": [
    "We are focusing on the *transcript* subset of this dataset, which contains the raw textual information about different companies. This is the basic structure of the dataset and serves as the starting point for any RAG or AI Agent architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the result to see the difference\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-2",
   "metadata": {},
   "source": [
    "There are a total of 188 transcripts in our data. These transcripts belong to different companies, and we need to count how many unique companies are represented in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through documents and value counts company\n",
    "company_counts = {}\n",
    "for doc in documents:\n",
    "    company = doc.metadata.get(\"company\")\n",
    "    if company:\n",
    "        company_counts[company] = company_counts.get(company, 0) + 1\n",
    "\n",
    "print(\"Total company counts:\")\n",
    "for company, count in company_counts.items():\n",
    "    print(f\" - {company}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-3",
   "metadata": {},
   "source": [
    "Almost all companies have equal distribution ratios. Take a look at the metadata of a random transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eadbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metadata of random document\n",
    "print(documents[0].metadata)\n",
    "print(documents[33].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-4",
   "metadata": {},
   "source": [
    "The **company** field simply indicates which company the transcript belongs to, and the **date** field represents the timeframe the information is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first doc 200 characters\n",
    "first_doc = documents[0]\n",
    "print(first_doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-5",
   "metadata": {},
   "source": [
    "By printing a sample of our document, we can get a high-level overview. For example, the current sample shows the quarterly report of AMD.\n",
    "\n",
    "Transcripts can be very long because they represent information for a given timeframe and contain a large amount of detail. We need to check how many words, on average, those 188 transcripts contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of words in first_doc\n",
    "first_doc = documents[0]\n",
    "num_words = len(first_doc.page_content.split())\n",
    "print(f\"Number of words in the first document: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff500bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check average number of words in all documents\n",
    "total_words = sum(len(doc.page_content.split()) for doc in documents)\n",
    "average_words = total_words / len(documents) if documents else 0\n",
    "print(f\"Average number of words in documents: {average_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-6",
   "metadata": {},
   "source": [
    "~9K words per transcript is quite huge, as it surely contains a large amount of information. But this is exactly what we need creating a well-structured knowledge base AI agent involves handling massive amounts of information, not just a few small documents.\n",
    "\n",
    "![Words Distribution Plot](https://miro.medium.com/v2/resize:fit:2000/1*_MeNr-2d3Bbr-ydc9fqv6g.png)\n",
    "\n",
    "Normally, financial data is based on different timeframes, each representing different information about what is happening during that period. We can extract those timeframes from the transcripts using plain Python code instead of LLMs to save costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5828460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def find_quarter(text: str) -> str | None:\n",
    "    \"\"\"Extract the quarter (e.g., 'Q1 2023') from the input text if present.\"\"\"\n",
    "    # uses this regex, which is consistent with the dataset\n",
    "    search_results = re.findall(r\"[Q]\\d\\s\\d{4}\", text)\n",
    "    if search_results:\n",
    "        return str(search_results[0])\n",
    "    return None\n",
    "\n",
    "# Let's test it on our first document\n",
    "quarter = find_quarter(documents[0].page_content)\n",
    "print(f\"Extracted Quarter for the first document: {quarter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-analysis-7",
   "metadata": {},
   "source": [
    "A better way to perform quarter-date extraction is through LLMs, as they can understand the data in a deeper way. However, since our data is already well-structured in terms of text, we can proceed without them for now.\n",
    "\n",
    "![Transcripts Quarter Analysis](https://miro.medium.com/v2/resize:fit:4800/1*00_ik7Mh17LOQ338EW7m0g.png)\n",
    "\n",
    "Now that we have a quick understanding of our dynamic data, we can start building the knowledge base through a temporal AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semantic-chunking-intro",
   "metadata": {},
   "source": [
    "## Percentile Semantic Chunking\n",
    "\n",
    "![Percentile Based Chunking](https://miro.medium.com/v2/resize:fit:2000/1*bcd_ko6FvfO53Cb46NJb7g.png)\n",
    "\n",
    "Normally, we chunk the data based on either random splits or at meaningful sentence boundaries, such as ending at a full stop. However, this approach might lead to losing some information. For example:\n",
    "\n",
    "> Net income rose 12% to $2.1M. The increase was driven by lower operating expenses\n",
    "\n",
    "If we split at the full stop, we lose the compact connection that the increase in net income was due to lower operating expenses.\n",
    "\n",
    "We will be working with percentile-based chunking here. Let’s first understand this approach and then implement it.\n",
    "\n",
    "![Percentile Chunking](https://miro.medium.com/v2/resize:fit:4532/1*5S2YB56b4BGFHfUcTre5pA.png)\n",
    "\n",
    "1.  The document is split into sentences using a regex, usually breaking after `.`, `?`, or `!`.\n",
    "2.  Each sentence is converted into a high-dimensional vector using the embedding model.\n",
    "3.  The semantic distance between consecutive sentence vectors is calculated, with larger values indicating bigger topic changes.\n",
    "4.  All distances are collected and the chosen percentile, such as the 95th, is determined to capture unusually large jumps.\n",
    "5.  Boundaries where the distance is greater than or equal to this threshold are marked as chunk breakpoints.\n",
    "6.  Sentences between these boundaries are grouped into chunks, applying `min_chunk_size` to avoid overly small chunks and `buffer_size` to add overlap if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eea1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nebius import NebiusEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set Nebius API key (⚠️ Avoid hardcoding secrets in production code)\n",
    "os.environ[\"NEBIUS_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# 1. Initialize the embedding model. We are using `Qwen3–8B` to generate embeddings through Nebius AI in LangChain.\n",
    "embeddings = NebiusEmbeddings(model=\"Qwen/Qwen3-Embedding-8B\")\n",
    "\n",
    "# 2. Instantiate LangChain's SemanticChunker\n",
    "# We have chosen the 95th percentile value, which means that if the distance between \n",
    "# consecutive sentences goes above this value, it will be considered a breakpoint.\n",
    "langchain_semantic_chunker = SemanticChunker(\n",
    "    embeddings, breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "# We'll store the new, smaller chunk documents here\n",
    "chunked_documents_lc = []\n",
    "\n",
    "# Filter for just AMD and NVDA documents (same as before)\n",
    "target_companies = [\"AMD\", \"NVDA\"]\n",
    "docs_to_process = [doc for doc in documents if doc.metadata.get(\"company\") in target_companies]\n",
    "\n",
    "# sample 1 document from the filtered list for a quick run\n",
    "docs_to_process = docs_to_process[:1]\n",
    "\n",
    "print(f\"Processing {len(docs_to_process)} documents using LangChain's SemanticChunker...\")\n",
    "\n",
    "# Loop through each transcript document and chunk it\n",
    "for doc in tqdm(docs_to_process, desc=\"Chunking Transcripts with LangChain\"):\n",
    "    # 1. Extract quarter and prepare metadata (same as before)\n",
    "    quarter = find_quarter(doc.page_content)\n",
    "    parent_metadata = doc.metadata.copy()\n",
    "    parent_metadata[\"quarter\"] = quarter\n",
    "\n",
    "    # 2. Perform semantic chunking using the LangChain chunker's `create_documents` method\n",
    "    # This is convenient as it directly returns Document objects.\n",
    "    # We pass the metadata to be attached to each new chunk.\n",
    "    chunks = langchain_semantic_chunker.create_documents([doc.page_content], metadatas=[parent_metadata])\n",
    "    \n",
    "    # 3. Add the returned chunk Documents to our list\n",
    "    chunked_documents_lc.extend(chunks)\n",
    "\n",
    "print(\"\\nChunking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semantic-chunking-analysis",
   "metadata": {},
   "source": [
    "We have, on average, 19 chunks per transcript. Let’s inspect a random chunk from one of our transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the LangChain chunking process\n",
    "original_doc_count = len(docs_to_process)\n",
    "chunked_doc_count = len(chunked_documents_lc)\n",
    "\n",
    "print(f\"Original number of documents (transcripts): {original_doc_count}\")\n",
    "print(f\"Number of new documents (chunks): {chunked_doc_count}\")\n",
    "print(f\"Average chunks per transcript: {chunked_doc_count / original_doc_count:.2f}\")\n",
    "\n",
    "print(\"\\n--- Inspecting a sample chunk ---\")\n",
    "sample_chunk = chunked_documents_lc[10] # Let's look at the 11th chunk\n",
    "print(\"Sample Chunk Content (first 300 chars):\")\n",
    "print(sample_chunk.page_content[:300] + \"...\")\n",
    "print(\"\\nSample Chunk Metadata:\")\n",
    "print(sample_chunk.metadata)\n",
    "\n",
    "# Let's check the average word count of the new chunks\n",
    "total_chunk_words = sum(len(doc.page_content.split()) for doc in chunked_documents_lc)\n",
    "average_chunk_words = total_chunk_words / chunked_doc_count if chunked_documents_lc else 0\n",
    "\n",
    "print(f\"\\nAverage number of words per chunk: {average_chunk_words:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-facts-intro",
   "metadata": {},
   "source": [
    "## Extracting Atomic Facts with a Statement Agent\n",
    "Now that we have our data neatly organized into small, meaningful chunks, we can begin using a LLM to read these chunks and pull out the core facts.\n",
    "\n",
    "![Atomic Facts Extraction](https://miro.medium.com/v2/resize:fit:2000/1*CoNzBT9NF17IJeUZigFUcA.png)\n",
    "\n",
    "> ### Why Extract Statements First?\n",
    "We need to break down the text into the smallest possible “atomic” facts. For example, instead of a single, complex sentence, we want individual claims that can stand on their own.\n",
    "\n",
    "![Atomic Facts Extraction Flow](https://miro.medium.com/v2/resize:fit:2000/1*ixe2itZX8vOnSp2BOiYEnQ.png)\n",
    "\n",
    "This process makes the information much easier for our AI system to understand, query, and reason with later on.\n",
    "\n",
    "To ensure our LLM gives us a clean, predictable output, we need to give it a strict set of instructions. The best way to do this in Python is with `Pydantic` models. These models act as a \"schema\" or a \"template\" that the LLM must follow.\n",
    "\n",
    "First, let’s define the allowed categories for our labels using Enums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, field_validator\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Since you're on Python 3.10, we'll define enums by inheriting from str and Enum.\n",
    "# This provides the same behavior as StrEnum from Python 3.11.\n",
    "class TemporalType(str, Enum):\n",
    "    \"\"\"Enumeration of temporal types of statements.\"\"\"\n",
    "    ATEMPORAL = \"ATEMPORAL\"\n",
    "    STATIC = \"STATIC\"\n",
    "    DYNAMIC = \"DYNAMIC\"\n",
    "\n",
    "class StatementType(str, Enum):\n",
    "    \"\"\"Enumeration of statement types for statements.\"\"\"\n",
    "    FACT = \"FACT\"\n",
    "    OPINION = \"OPINION\"\n",
    "    PREDICTION = \"PREDICTION\"\n",
    "\n",
    "class RawStatement(BaseModel):\n",
    "    \"\"\"Model representing a raw statement with type and temporal information.\"\"\"\n",
    "    statement: str\n",
    "    statement_type: StatementType\n",
    "    temporal_type: TemporalType\n",
    "\n",
    "    @field_validator(\"temporal_type\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _parse_temporal_label(cls, value: str | None) -> TemporalType:\n",
    "        if value is None:\n",
    "            return TemporalType.ATEMPORAL\n",
    "        cleaned_value = value.strip().upper()\n",
    "        try:\n",
    "            return TemporalType(cleaned_value)\n",
    "        except ValueError:\n",
    "            # Pydantic will catch this and raise a validation error\n",
    "            raise ValueError(f\"Invalid temporal type: {value}\")\n",
    "\n",
    "    @field_validator(\"statement_type\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _parse_statement_label(cls, value: str | None = None) -> StatementType:\n",
    "        if value is None:\n",
    "            return StatementType.FACT\n",
    "        cleaned_value = value.strip().upper()\n",
    "        try:\n",
    "            return StatementType(cleaned_value)\n",
    "        except ValueError:\n",
    "            # Pydantic will catch this and raise a validation error\n",
    "            raise ValueError(f\"Invalid statement type: {value}\")\n",
    "\n",
    "class RawStatementList(BaseModel):\n",
    "    \"\"\"A container for the list of statements extracted from a single chunk.\"\"\"\n",
    "    statements: list[RawStatement]\n",
    "\n",
    "print(\"Pydantic models for statement extraction have been defined and are Python 3.10 compatible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-facts-prompting",
   "metadata": {},
   "source": [
    "![Atomic Facts Extraction](https://miro.medium.com/v2/resize:fit:2000/1*rln0caB7JDFJ8tkSclMTrw.png)\n",
    "\n",
    "Let’s create the contextual definitions we will provide to the LLM about our labels. This helps it understand the difference between a `FACT` and an `OPINION`, or a `STATIC` and `DYNAMIC` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These definitions provide the necessary context for the LLM to understand the labels.\n",
    "LABEL_DEFINITIONS: dict[str, dict[str, dict[str, str]]] = {\n",
    "    \"episode_labelling\": {\n",
    "        \"FACT\": dict(definition=\"Statements that are objective and can be independently verified or falsified through evidence.\"),\n",
    "        \"OPINION\": dict(definition=\"Statements that contain personal opinions, feelings, values, or judgments that are not independently verifiable.\"),\n",
    "        \"PREDICTION\": dict(definition=\"Uncertain statements about the future on something that might happen, a hypothetical outcome, unverified claims.\"),\n",
    "    },\n",
    "    \"temporal_labelling\": {\n",
    "        \"STATIC\": dict(definition=\"Often past tense, think -ed verbs, describing single points-in-time.\"),\n",
    "        \"DYNAMIC\": dict(definition=\"Often present tense, think -ing verbs, describing a period of time.\"),\n",
    "        \"ATEMPORAL\": dict(definition=\"Statements that will always hold true regardless of time.\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# This is the corrected prompt template. Notice the double curly braces {{ and }} in the JSON example.\n",
    "statement_extraction_prompt_template_fixed = \"\"\"\n",
    "You are an expert finance professional and information-extraction assistant.\n",
    "\n",
    "===Inputs===\n",
    "- main_entity: {main_entity}\n",
    "- publication_date: {publication_date}\n",
    "- document_chunk: {document_chunk}\n",
    "\n",
    "===Tasks===\n",
    "1. Identify and extract atomic declarative statements from the document_chunk.\n",
    "2. For each statement, label it as FACT, OPINION, or PREDICTION.\n",
    "3. For each statement, label it temporally as STATIC, DYNAMIC, or ATEMPORAL.\n",
    "\n",
    "===Extraction Guidelines===\n",
    "- Each statement should express a single, complete subject-predicate-object relationship.\n",
    "- Resolve co-references (e.g., \"the company\" -> \"{main_entity}\").\n",
    "- Include any explicit dates or quantitative qualifiers.\n",
    "\n",
    "===Label Definitions===\n",
    "{definitions}\n",
    "\n",
    "===Example===\n",
    "Chunk: \"On April 1st, 2024, John Smith was appointed CFO of TechNova Inc. He is currently overseeing the company’s global restructuring initiative.\"\n",
    "Output: {{\n",
    "  \"statements\": [\n",
    "    {{\n",
    "      \"statement\": \"John Smith was appointed CFO of TechNova Inc on April 1st, 2024.\",\n",
    "      \"statement_type\": \"FACT\",\n",
    "      \"temporal_type\": \"STATIC\"\n",
    "    }},\n",
    "    {{\n",
    "      \"statement\": \"John Smith is currently overseeing TechNova Inc's global restructuring initiative.\",\n",
    "      \"statement_type\": \"FACT\",\n",
    "      \"temporal_type\": \"DYNAMIC\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "===End of Example===\n",
    "\n",
    "**Output format**\n",
    "Return ONLY a valid JSON object matching the schema for `RawStatementList`.\n",
    "\"\"\"\n",
    "\n",
    "# Create the ChatPromptTemplate from the fixed string.\n",
    "prompt = ChatPromptTemplate.from_template(statement_extraction_prompt_template_fixed)\n",
    "\n",
    "# We still need to format the definitions_text to pass into the invoke call.\n",
    "definitions_text = \"\"\n",
    "for section_key, section_dict in LABEL_DEFINITIONS.items():\n",
    "    definitions_text += f\"==== {section_key.replace('_', ' ').upper()} DEFINITIONS ====\\n\"\n",
    "    for category, details in section_dict.items():\n",
    "        definitions_text += f\"- {category}: {details.get('definition', '')}\\n\"\n",
    "\n",
    "print(\"Prompt template for statement extraction has been correctly created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-facts-testing",
   "metadata": {},
   "source": [
    "Finally, we connect everything. We will create a LangChain “chain” that links our prompt to the LLM and tells the LLM to structure its output according to our `RawStatementList` model.\n",
    "\n",
    "We will use the `deepseek-ai/DeepSeek-V3` model via Nebius for this task, as it's powerful and good at following complex instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nebius import ChatNebius\n",
    "import json\n",
    "\n",
    "# We'll use a powerful model as recommended by the notebook for this complex extraction task.\n",
    "llm = ChatNebius(model=\"deepseek-ai/DeepSeek-V3\")\n",
    "statement_extraction_chain = prompt | llm.with_structured_output(RawStatementList)\n",
    "\n",
    "# Let's use the first chunk for extraction\n",
    "sample_chunk_for_extraction = chunked_documents_lc[0]\n",
    "\n",
    "print(\"--- Running statement extraction on a sample chunk (with fixed prompt) ---\")\n",
    "print(f\"Chunk Content:\\n{sample_chunk_for_extraction.page_content}\")\n",
    "print(\"\\nInvoking LLM for extraction...\")\n",
    "\n",
    "# Run the chain, passing ALL variables, including definitions, in the dictionary.\n",
    "extracted_statements_list = statement_extraction_chain.invoke({\n",
    "    \"main_entity\": sample_chunk_for_extraction.metadata[\"company\"],\n",
    "    \"publication_date\": sample_chunk_for_extraction.metadata[\"date\"].isoformat(),\n",
    "    \"document_chunk\": sample_chunk_for_extraction.page_content,\n",
    "    \"definitions\": definitions_text # Pass the definitions text here\n",
    "})\n",
    "\n",
    "print(\"\\n--- Extraction Result ---\")\n",
    "print(extracted_statements_list.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-agent-intro",
   "metadata": {},
   "source": [
    "## Pinpointing Time with a Validation Check Agent\n",
    "\n",
    "We have successfully extracted the **what** from our text the atomic statements. Now, we need to extract the **when**. Each statement needs a precise timestamp to tell us when it was valid.\n",
    "\n",
    "![Validation Check Process](https://miro.medium.com/v2/resize:fit:2000/1*7Yh_2ucfZS4-Si9gGtjePA.png)\n",
    "\n",
    "This is the most important step in making our knowledge base truly “temporal”.\n",
    "\n",
    "> ### Why We Need a Dedicated Date Agent?\n",
    "\n",
    "Extracting dates from natural language is tricky. A statement might say **next quarter**, **three months ago** or **in 2017**. Humans understand this, but a computer needs a concrete date, like `2017-01-01`.\n",
    "\n",
    "![Valid/Invalid Fact Check Flow](https://miro.medium.com/v2/resize:fit:3848/1*JNb3cpItvRBHo01rybibGQ.png)\n",
    "\n",
    "Our goal is to create a specialized agent that can read a statement and, using the original document’s publication date as a reference, figure out two key timestamps:\n",
    "\n",
    "- `valid_at`: The date the fact became true.\n",
    "- `invalid_at`: The date the fact stopped being true (if it's no longer valid).\n",
    "\n",
    "Just like before, we’ll start by defining Pydantic models to ensure our LLM gives us a clean, structured output for the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aed366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pydantic import Field\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# This helper function is crucial for robust date parsing and is Python 3.10 compatible.\n",
    "def parse_date_str(value: str | datetime | None) -> datetime | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    if isinstance(value, datetime):\n",
    "        return value if value.tzinfo else value.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        # Handle simple YYYY format\n",
    "        if re.fullmatch(r\"\\\\d{4}\", value.strip()):\n",
    "            year = int(value.strip())\n",
    "            return datetime(year, 1, 1, tzinfo=timezone.utc)\n",
    "        # General purpose parsing\n",
    "        dt: datetime = parse(value)\n",
    "        # Assume UTC if no timezone is present\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# These Pydantic models will structure the output of our date extraction LLM call.\n",
    "class RawTemporalRange(BaseModel):\n",
    "    \"\"\"Model representing the raw temporal validity range as strings from the LLM.\"\"\"\n",
    "    valid_at: str | None = Field(None, description=\"The start date/time of the event's validity in ISO 8601 format.\")\n",
    "    invalid_at: str | None = Field(None, description=\"The end date/time of the event's validity in ISO 8601 format.\")\n",
    "\n",
    "class TemporalValidityRange(BaseModel):\n",
    "    \"\"\"Model representing the parsed and validated temporal range as datetime objects.\"\"\"\n",
    "    valid_at: datetime | None = None\n",
    "    invalid_at: datetime | None = None\n",
    "\n",
    "    @field_validator(\"valid_at\", \"invalid_at\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _parse_date_string(cls, value: str | datetime | None) -> datetime | None:\n",
    "        # This validator automatically uses our robust parse_date_str function.\n",
    "        return parse_date_str(value)\n",
    "\n",
    "print(\"Pydantic models and helper function for temporal range extraction are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-agent-prompting",
   "metadata": {},
   "source": [
    "Next, we create a new prompt specifically for this date extraction task. We will give the LLM one of our extracted statements and ask it to determine the validity dates based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_extraction_prompt_template = \"\"\"\n",
    "You are a temporal information extraction specialist.\n",
    "\n",
    "INPUTS:\n",
    "- statement: \"{statement}\"\n",
    "- statement_type: \"{statement_type}\"\n",
    "- temporal_type: \"{temporal_type}\"\n",
    "- publication_date: \"{publication_date}\"\n",
    "- quarter: \"{quarter}\"\n",
    "\n",
    "TASK:\n",
    "- Analyze the statement and determine the temporal validity range (valid_at, invalid_at).\n",
    "- Use the publication date as the reference point for relative expressions (e.g., \"last month\", \"currently\").\n",
    "- If a relationship is ongoing or its end is not specified, `invalid_at` should be null.\n",
    "\n",
    "GUIDANCE:\n",
    "- For STATIC statements, `invalid_at` is usually null. The `valid_at` is the date the event occurred.\n",
    "- For DYNAMIC statements, `valid_at` is when the state began, and `invalid_at` is when it ended.\n",
    "- Return all dates in ISO 8601 format (e.g., YYYY-MM-DDTHH:MM:SSZ).\n",
    "- If only a year is mentioned, use the first day of that year (YYYY-01-01T00:00:00Z).\n",
    "\n",
    "**Output format**\n",
    "Return ONLY a valid JSON object matching the schema for `RawTemporalRange`.\n",
    "\"\"\"\n",
    "\n",
    "# Create the LangChain prompt and chain for date extraction.\n",
    "date_extraction_prompt = ChatPromptTemplate.from_template(date_extraction_prompt_template)\n",
    "date_extraction_chain = date_extraction_prompt | llm.with_structured_output(RawTemporalRange)\n",
    "\n",
    "print(\"Date extraction chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-agent-testing",
   "metadata": {},
   "source": [
    "Now we build the LangChain chain for this step and test it on one of the statements we extracted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b53ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the second statement from our previous result: \"Lisa Su is the President and CEO of AMD.\"\n",
    "# The original model classified it as STATIC, let's see how our date extraction handles it.\n",
    "sample_statement = extracted_statements_list.statements[1]\n",
    "chunk_metadata = sample_chunk_for_extraction.metadata\n",
    "\n",
    "print(f\"--- Running date extraction for statement ---\")\n",
    "print(f'Statement: \"{sample_statement.statement}\"')\n",
    "print(f\"Statement Type: {sample_statement.statement_type.value}\")\n",
    "print(f\"Temporal Type: {sample_statement.temporal_type.value}\")\n",
    "print(f\"Reference Publication Date: {chunk_metadata['date'].isoformat()}\")\n",
    "\n",
    "# Invoke the date extraction chain with the necessary inputs\n",
    "raw_temporal_range = date_extraction_chain.invoke({\n",
    "    \"statement\": sample_statement.statement,\n",
    "    \"statement_type\": sample_statement.statement_type.value,\n",
    "    \"temporal_type\": sample_statement.temporal_type.value,\n",
    "    \"publication_date\": chunk_metadata[\"date\"].isoformat(),\n",
    "    \"quarter\": chunk_metadata[\"quarter\"]\n",
    "})\n",
    "\n",
    "# Validate and parse the raw LLM output into our final datetime model\n",
    "final_temporal_range = TemporalValidityRange.model_validate(raw_temporal_range.model_dump())\n",
    "\n",
    "print(\"\\n--- Parsed & Validated Result (with fix) ---\")\n",
    "print(f\"Valid At: {final_temporal_range.valid_at}\")\n",
    "print(f\"Invalid At: {final_temporal_range.invalid_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "triplet-extraction-intro",
   "metadata": {},
   "source": [
    "## Structuring Facts into Triplets\n",
    "\n",
    "We have successfully extracted what the facts are (the statements) and when they were true (the dates).\n",
    "\n",
    "![Triplets Extraction](https://miro.medium.com/v2/resize:fit:2000/1*Uvtah57u3izKyiS19ocQGg.png)\n",
    "\n",
    "Now, we need to convert these natural language sentences into a format that an AI agent can easily understand and connect.\n",
    "\n",
    "![Triplet Extraction Flow](https://miro.medium.com/v2/resize:fit:2000/1*soSwKELDsjsyv-6Brza5GA.png)\n",
    "\n",
    "A triplet breaks down a fact into three core components:\n",
    "1.  **Subject:** The main entity the fact is about.\n",
    "2.  **Predicate:** The relationship or action.\n",
    "3.  **Object:** The entity or concept the subject is related to.\n",
    "\n",
    "By converting all our statements into this format, we can build a web of interconnected facts our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb49704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pydantic import Field\n",
    "\n",
    "# We'll use the Python 3.10 compatible (str, Enum) for the Predicate.\n",
    "class Predicate(str, Enum):\n",
    "    \"\"\"Enumeration of normalised predicates for knowledge graph relationships.\"\"\"\n",
    "    IS_A = \"IS_A\"\n",
    "    HAS_A = \"HAS_A\"\n",
    "    LOCATED_IN = \"LOCATED_IN\"\n",
    "    HOLDS_ROLE = \"HOLDS_ROLE\"\n",
    "    PRODUCES = \"PRODUCES\"\n",
    "    SELLS = \"SELLS\"\n",
    "    LAUNCHED = \"LAUNCHED\"\n",
    "    DEVELOPED = \"DEVELOPED\"\n",
    "    ADOPTED_BY = \"ADOPTED_BY\"\n",
    "    INVESTS_IN = \"INVESTS_IN\"\n",
    "    COLLABORATES_WITH = \"COLLABORATES_WITH\"\n",
    "    SUPPLIES = \"SUPPLIES\"\n",
    "    HAS_REVENUE = \"HAS_REVENUE\"\n",
    "    INCREASED = \"INCREASED\"\n",
    "    DECREASED = \"DECREASED\"\n",
    "    RESULTED_IN = \"RESULTED_IN\"\n",
    "    TARGETS = \"TARGETS\"\n",
    "    PART_OF = \"PART_OF\"\n",
    "    DISCONTINUED = \"DISCONTINUED\"\n",
    "    SECURED = \"SECURED\"\n",
    "\n",
    "class RawEntity(BaseModel):\n",
    "    \"\"\"Represents an entity as initially extracted by the LLM.\"\"\"\n",
    "    entity_idx: int = Field(description=\"A temporary, 0-indexed ID for entities within this extraction.\")\n",
    "    name: str = Field(description=\"The name of the entity, e.g., 'AMD' or 'Lisa Su'.\")\n",
    "    type: str = Field(\"Unknown\", description=\"The type of entity, e.g., 'Organization', 'Person', 'Product'.\")\n",
    "    description: str = Field(\"\", description=\"A brief, context-independent description of the entity.\")\n",
    "\n",
    "class RawTriplet(BaseModel):\n",
    "    \"\"\"Represents a subject-predicate-object relationship.\"\"\"\n",
    "    subject_name: str\n",
    "    subject_id: int = Field(description=\"The entity_idx of the subject.\")\n",
    "    predicate: Predicate\n",
    "    object_name: str\n",
    "    object_id: int = Field(description=\"The entity_idx of the object.\")\n",
    "    value: str | None = Field(None, description=\"An optional value associated with the relationship, e.g., '10%'.\")\n",
    "\n",
    "class RawExtraction(BaseModel):\n",
    "    \"\"\"A container for the complete extraction of entities and triplets from a statement.\"\"\"\n",
    "    entities: list[RawEntity]\n",
    "    triplets: list[RawTriplet]\n",
    "\n",
    "print(\"Pydantic models for triplet extraction have been defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "triplet-extraction-prompting",
   "metadata": {},
   "source": [
    "Next, we create the prompt and the definitions that will guide the LLM. This prompt is highly specific, instructing the model to focus only on the relationships and ignore any time-based information, which we have already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe357b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These definitions from the notebook guide the LLM in choosing the correct predicate.\n",
    "PREDICATE_DEFINITIONS = {\n",
    "    \"IS_A\": \"Denotes a class-or-type relationship (e.g., 'Model Y IS_A electric-SUV').\",\n",
    "    \"HAS_A\": \"Denotes a part-whole relationship (e.g., 'Model Y HAS_A electric-engine').\",\n",
    "    \"LOCATED_IN\": \"Specifies geographic or organisational containment.\",\n",
    "    \"HOLDS_ROLE\": \"Connects a person to a formal office or title.\",\n",
    "    \"PRODUCES\": \"Indicates that an entity manufactures or creates a product.\",\n",
    "    \"SELLS\": \"Marks a commercial seller-to-customer relationship.\",\n",
    "    \"LAUNCHED\": \"Captures the official first release of a product or service.\",\n",
    "    \"DEVELOPED\": \"Shows design or R&D origin of a technology or product.\",\n",
    "    \"ADOPTED_BY\": \"Indicates that a technology has been taken up by another entity.\",\n",
    "    \"INVESTS_IN\": \"Represents the flow of capital from one entity to another.\",\n",
    "    \"COLLABORATES_WITH\": \"Generic partnership or joint venture.\",\n",
    "    \"SUPPLIES\": \"Captures vendor–client supply-chain links.\",\n",
    "    \"HAS_REVENUE\": \"Associates an entity with a revenue amount.\",\n",
    "    \"INCREASED\": \"Expresses an upward change in a metric.\",\n",
    "    \"DECREASED\": \"Expresses a downward change in a metric.\",\n",
    "    \"RESULTED_IN\": \"Captures a causal relationship.\",\n",
    "    \"TARGETS\": \"Denotes a strategic objective or market segment.\",\n",
    "    \"PART_OF\": \"Expresses hierarchical membership (e.g., a division is PART_OF a company).\",\n",
    "    \"DISCONTINUED\": \"Indicates end-of-life for a product or service.\",\n",
    "    \"SECURED\": \"Marks the successful acquisition of funding, contracts, etc.\"\n",
    "}\n",
    "\n",
    "# The prompt template for extracting entities and triplets.\n",
    "# We are careful to escape the JSON example with {{ and }}.\n",
    "triplet_extraction_prompt_template = \"\"\"\n",
    "You are an information-extraction assistant.\n",
    "\n",
    "**Task:** From the given statement, first identify all entities (people, organizations, products, concepts). Then, identify all triplets (subject, predicate, object) that describe the relationships between these entities.\n",
    "\n",
    "**Statement:** \"{statement}\"\n",
    "\n",
    "**Predicate Instructions:**\n",
    "You must use a predicate from the following list.\n",
    "{predicate_instructions}\n",
    "\n",
    "**Guidelines:**\n",
    "- First, list all identified entities with a unique `entity_idx`.\n",
    "- Second, list all triplets, using the `entity_idx` to link subjects and objects.\n",
    "- Exclude all temporal expressions (dates, years, etc.) from the entities and triplets.\n",
    "\n",
    "===Example===\n",
    "Statement: \"Google's revenue increased by 10% from January through March.\"\n",
    "Output: {{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"entity_idx\": 0,\n",
    "      \"name\": \"Google\",\n",
    "      \"type\": \"Organization\",\n",
    "      \"description\": \"A multinational technology company.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"entity_idx\": 1,\n",
    "      \"name\": \"Revenue\",\n",
    "      \"type\": \"Financial Metric\",\n",
    "      \"description\": \"The income generated from normal business operations.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"triplets\": [\n",
    "    {{\n",
    "      \"subject_name\": \"Google\",\n",
    "      \"subject_id\": 0,\n",
    "      \"predicate\": \"INCREASED\",\n",
    "      \"object_name\": \"Revenue\",\n",
    "      \"object_id\": 1,\n",
    "      \"value\": \"10%\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "===End of Example===\n",
    "\n",
    "**Output format**\n",
    "Return ONLY a valid JSON object that follows the schema of the `RawExtraction` type.\n",
    "\"\"\"\n",
    "\n",
    "# Format the predicate instructions for the prompt.\n",
    "predicate_instructions_text = \"\\n\".join(f\"- {pred}: {desc}\" for pred, desc in PREDICATE_DEFINITIONS.items())\n",
    "\n",
    "# Create the final prompt template.\n",
    "triplet_extraction_prompt = ChatPromptTemplate.from_template(\n",
    "    triplet_extraction_prompt_template\n",
    ")\n",
    "\n",
    "print(\"Prompt template for triplet extraction created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "triplet-extraction-testing",
   "metadata": {},
   "source": [
    "Finally, we create our third chain and test it on one of our statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb32dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain for triplet and entity extraction.\n",
    "triplet_extraction_chain = triplet_extraction_prompt | llm.with_structured_output(RawExtraction)\n",
    "\n",
    "# We'll use the same sample statement from the previous steps.\n",
    "# Statement: \"Lisa Su is the President and CEO of AMD.\"\n",
    "sample_statement_for_triplets = extracted_statements_list.statements[1]\n",
    "\n",
    "print(f\"--- Running triplet extraction for statement ---\")\n",
    "print(f'Statement: \"{sample_statement_for_triplets.statement}\"')\n",
    "\n",
    "# Invoke the chain.\n",
    "raw_extraction_result = triplet_extraction_chain.invoke({\n",
    "    \"statement\": sample_statement_for_triplets.statement,\n",
    "    \"predicate_instructions\": predicate_instructions_text\n",
    "})\n",
    "\n",
    "print(\"\\n--- Triplet Extraction Result ---\")\n",
    "# Using .model_dump_json for a clean, indented JSON output.\n",
    "print(raw_extraction_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-event-intro",
   "metadata": {},
   "source": [
    "## Assembling the Temporal Event\n",
    "\n",
    "We have now completed all the individual extraction steps. We have a system that can take a piece of text and extract:\n",
    "\n",
    "1.  **Statements**: The atomic facts.\n",
    "2.  **Dates**: The “when” for each fact.\n",
    "3.  **Entities & Triplets**: The “who” and “what” in a structured format.\n",
    "\n",
    "The final step in our extraction process is to bring all these pieces together. We will create a master data model called a **TemporalEvent** that consolidates all the information about a single statement into one clean, unified object.\n",
    "\n",
    "![Temporal Event](https://miro.medium.com/v2/resize:fit:2000/1*SuWIfM0mkNJBa7k1ZG0MLw.png)\n",
    "\n",
    "This `TemporalEvent` will be the central object we work with for the rest of the ingestion pipeline. It will hold everything: the original statement, its type, its temporal range, and links to all the triplets that were derived from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a514972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from pydantic import model_validator\n",
    "\n",
    "# They will replace the \"Raw\" versions for our final data structure.\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    \"\"\"The final, persistent representation of an entity.\"\"\"\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    resolved_id: uuid.UUID | None = None # For entity resolution later\n",
    "\n",
    "class Triplet(BaseModel):\n",
    "    \"\"\"The final, persistent representation of a triplet.\"\"\"\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
    "    subject_name: str\n",
    "    subject_id: uuid.UUID\n",
    "    predicate: Predicate\n",
    "    object_name: str\n",
    "    object_id: uuid.UUID\n",
    "    value: str | None = None\n",
    "\n",
    "class TemporalEvent(BaseModel):\n",
    "    \"\"\"\n",
    "    The central model that consolidates all extracted information for a single statement.\n",
    "    \"\"\"\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
    "    chunk_id: uuid.UUID # We will add this later when processing the full chunk\n",
    "    statement: str\n",
    "    embedding: list[float] = [] # We will add this later\n",
    "    \n",
    "    # Temporal Information\n",
    "    statement_type: StatementType\n",
    "    temporal_type: TemporalType\n",
    "    valid_at: datetime | None = None\n",
    "    invalid_at: datetime | None = None\n",
    "    \n",
    "    # Link to triplets\n",
    "    triplets: list[uuid.UUID]\n",
    "    \n",
    "    # Metadata for invalidation\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    expired_at: datetime | None = None\n",
    "    invalidated_by: uuid.UUID | None = None\n",
    "\n",
    "    # This is a helper from the original model for DB storage\n",
    "    @property\n",
    "    def triplets_json(self) -> str:\n",
    "        \"\"\"Convert triplets list to a JSON string for database storage.\"\"\"\n",
    "        return json.dumps([str(t) for t in self.triplets]) if self.triplets else \"[]\"\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_expired_at(self) -> \"TemporalEvent\":\n",
    "        \"\"\"Set expired_at if invalid_at is set and temporal_type is DYNAMIC.\"\"\"\n",
    "        if self.invalid_at is not None and self.temporal_type == TemporalType.DYNAMIC:\n",
    "            self.expired_at = self.created_at\n",
    "        return self\n",
    "\n",
    "print(\"Final data models (Entity, Triplet, TemporalEvent) have been defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-event-assembly",
   "metadata": {},
   "source": [
    "Let’s manually assemble a single `TemporalEvent` to see how all the pieces fit together. We'll use the results from the previous steps for our sample statement.\n",
    "\n",
    "First, we will convert our `RawEntity` and `RawTriplet` objects into their final, persistent `Entity` and `Triplet` forms, complete with unique UUIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb648754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's gather all the pieces we have for our sample statement\n",
    "sample_statement = extracted_statements_list.statements[1]\n",
    "final_temporal_range = final_temporal_range # From the previous step\n",
    "raw_extraction_result = raw_extraction_result # From the previous step\n",
    "\n",
    "print(\"--- Assembling the final TemporalEvent ---\")\n",
    "\n",
    "# 1. Create persistent Entity objects from the raw extraction\n",
    "# We'll map the temporary entity_idx to the new, persistent UUID\n",
    "idx_to_entity_map: dict[int, Entity] = {}\n",
    "final_entities: list[Entity] = []\n",
    "\n",
    "for raw_entity in raw_extraction_result.entities:\n",
    "    entity = Entity(\n",
    "        name=raw_entity.name,\n",
    "        type=raw_entity.type,\n",
    "        description=raw_entity.description\n",
    "    )\n",
    "    idx_to_entity_map[raw_entity.entity_idx] = entity\n",
    "    final_entities.append(entity)\n",
    "    \n",
    "print(f\"Created {len(final_entities)} persistent Entity objects.\")\n",
    "\n",
    "# 2. Create persistent Triplet objects, linking them via the new UUIDs\n",
    "final_triplets: list[Triplet] = []\n",
    "for raw_triplet in raw_extraction_result.triplets:\n",
    "    # Look up the subject and object entities in our map\n",
    "    subject_entity = idx_to_entity_map[raw_triplet.subject_id]\n",
    "    object_entity = idx_to_entity_map[raw_triplet.object_id]\n",
    "    \n",
    "    triplet = Triplet(\n",
    "        subject_name=raw_triplet.subject_name,\n",
    "        subject_id=subject_entity.id,\n",
    "        predicate=raw_triplet.predicate,\n",
    "        object_name=raw_triplet.object_name,\n",
    "        object_id=object_entity.id,\n",
    "        value=raw_triplet.value\n",
    "    )\n",
    "    final_triplets.append(triplet)\n",
    "\n",
    "print(f\"Created {len(final_triplets)} persistent Triplet objects.\")\n",
    "\n",
    "# 3. Create the final TemporalEvent object\n",
    "# We'll generate a dummy chunk_id for now, as we haven't assigned them yet.\n",
    "temporal_event = TemporalEvent(\n",
    "    chunk_id=uuid.uuid4(), # Placeholder ID\n",
    "    statement=sample_statement.statement,\n",
    "    statement_type=sample_statement.statement_type,\n",
    "    temporal_type=sample_statement.temporal_type,\n",
    "    valid_at=final_temporal_range.valid_at,\n",
    "    invalid_at=final_temporal_range.invalid_at,\n",
    "    triplets=[t.id for t in final_triplets]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Assembled TemporalEvent ---\")\n",
    "print(temporal_event.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Associated Entities ---\")\n",
    "for entity in final_entities:\n",
    "    print(entity.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Associated Triplets ---\")\n",
    "for triplet in final_triplets:\n",
    "    print(triplet.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's gather all the pieces we have for our sample statement\n",
    "sample_statement = extracted_statements_list.statements[1]\n",
    "final_temporal_range = final_temporal_range # From the previous step\n",
    "raw_extraction_result = raw_extraction_result # From the previous step\n",
    "\n",
    "print(\"--- Assembling the final TemporalEvent ---\")\n",
    "\n",
    "# 1. Create persistent Entity objects from the raw extraction\n",
    "# We'll map the temporary entity_idx to the new, persistent UUID\n",
    "idx_to_entity_map: dict[int, Entity] = {}\n",
    "final_entities: list[Entity] = []\n",
    "\n",
    "for raw_entity in raw_extraction_result.entities:\n",
    "    entity = Entity(\n",
    "        name=raw_entity.name,\n",
    "        type=raw_entity.type,\n",
    "        description=raw_entity.description\n",
    "    )\n",
    "    idx_to_entity_map[raw_entity.entity_idx] = entity\n",
    "    final_entities.append(entity)\n",
    "    \n",
    "print(f\"Created {len(final_entities)} persistent Entity objects.\")\n",
    "\n",
    "# 2. Create persistent Triplet objects, linking them via the new UUIDs\n",
    "final_triplets: list[Triplet] = []\n",
    "for raw_triplet in raw_extraction_result.triplets:\n",
    "    # Look up the subject and object entities in our map\n",
    "    subject_entity = idx_to_entity_map[raw_triplet.subject_id]\n",
    "    object_entity = idx_to_entity_map[raw_triplet.object_id]\n",
    "    \n",
    "    triplet = Triplet(\n",
    "        subject_name=raw_triplet.subject_name,\n",
    "        subject_id=subject_entity.id,\n",
    "        predicate=raw_triplet.predicate,\n",
    "        object_name=raw_triplet.object_name,\n",
    "        object_id=object_entity.id,\n",
    "        value=raw_triplet.value\n",
    "    )\n",
    "    final_triplets.append(triplet)\n",
    "\n",
    "print(f\"Created {len(final_triplets)} persistent Triplet objects.\")\n",
    "\n",
    "# 3. Create the final TemporalEvent object\n",
    "# We'll generate a dummy chunk_id for now, as we haven't assigned them yet.\n",
    "temporal_event = TemporalEvent(\n",
    "    chunk_id=uuid.uuid4(), # Placeholder ID\n",
    "    statement=sample_statement.statement,\n",
    "    statement_type=sample_statement.statement_type,\n",
    "    temporal_type=sample_statement.temporal_type,\n",
    "    valid_at=final_temporal_range.valid_at,\n",
    "    invalid_at=final_temporal_range.invalid_at,\n",
    "    triplets=[t.id for t in final_triplets]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Assembled TemporalEvent ---\")\n",
    "print(temporal_event.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Associated Entities ---\")\n",
    "for entity in final_entities:\n",
    "    print(entity.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Associated Triplets ---\")\n",
    "for triplet in final_triplets:\n",
    "    print(triplet.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-automation-intro",
   "metadata": {},
   "source": [
    "## Automating the Pipeline with LangGraph\n",
    "So far, we have built three powerful, specialized “agents” (or chains): one to extract statements, one for dates, and one for triplets. We’ve also seen how to manually combine their outputs into a final `TemporalEvent`.\n",
    "\n",
    "But doing this manually for every statement in our thousands of chunks would be impossible. We need to automate this assembly line. This is where **LangGraph** comes in.\n",
    "\n",
    "> ### What is LangGraph and Why Use It?\n",
    "\n",
    "LangGraph is a library from LangChain for building complex, stateful AI applications. Instead of a simple `prompt -> LLM` chain, we can build a *graph* where each step is a \"node\". The information, or \"state\" flows from one node to the next.\n",
    "\n",
    "This is perfect for our use case. We can create a graph where:\n",
    "\n",
    "![LangGraph Temporal Event](https://miro.medium.com/v2/resize:fit:2000/1*oMhOQ3wu-cAmrivZcYGNIw.png)\n",
    "\n",
    "1.  The first node extracts statements from all chunks.\n",
    "2.  The second node extracts dates for all those statements.\n",
    "3.  The third node extracts triplets.\n",
    "4.  And so on…\n",
    "\n",
    "This creates a robust, repeatable, and easy-to-debug pipeline for processing our data.\n",
    "\n",
    "The first step in building a LangGraph is to define its “state.” The state is the memory of the graph, it’s the data that gets passed between each node. We’ll define a state that can hold our list of chunks and all the new objects we create along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "# We're using TypedDict for a clear, typed state definition.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        chunks: The list of document chunks to process.\n",
    "        temporal_events: A list of all extracted TemporalEvent objects.\n",
    "        entities: A list of all extracted Entity objects.\n",
    "        triplets: A list of all extracted Triplet objects.\n",
    "    \"\"\"\n",
    "    chunks: List[Document]\n",
    "    temporal_events: List[TemporalEvent]\n",
    "    entities: List[Entity]\n",
    "    triplets: List[Triplet]\n",
    "\n",
    "print(\"LangGraph state definition created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-automation-node",
   "metadata": {},
   "source": [
    "Now, we’ll combine all our previous logic into a single, powerful function. This function will be the main “node” in our graph. It takes the list of chunks from the state and orchestrates the entire extraction process in three parallel steps.\n",
    "\n",
    "A key optimization here is using the `.batch()` method. Instead of processing statements one by one in a slow loop, `.batch()` sends all of them to the LLM at once, which is much faster and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_events_from_chunks(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    A node that takes a list of chunks and performs the full extraction pipeline.\n",
    "    \"\"\"\n",
    "    print(f\"--- Entering Node: extract_events_from_chunks ---\")\n",
    "    chunks = state[\"chunks\"]\n",
    "    print(f\"Processing {len(chunks)} chunks...\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 1: Extract RawStatements from all chunks in a batch\n",
    "    # =========================================================================\n",
    "    statement_chain_inputs = [\n",
    "        {\n",
    "            \"main_entity\": chunk.metadata[\"company\"],\n",
    "            \"publication_date\": chunk.metadata[\"date\"].isoformat(),\n",
    "            \"document_chunk\": chunk.page_content,\n",
    "            \"definitions\": definitions_text\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    all_raw_statement_lists = statement_extraction_chain.batch(statement_chain_inputs)\n",
    "    \n",
    "    # We need to flatten the list and associate each statement with its original chunk\n",
    "    statements_with_chunk_info = []\n",
    "    for i, raw_statement_list in enumerate(all_raw_statement_lists):\n",
    "        chunk_id = uuid.uuid4() # Assign a unique ID to each chunk for tracking\n",
    "        for raw_statement in raw_statement_list.statements:\n",
    "            statements_with_chunk_info.append({\n",
    "                \"raw_statement\": raw_statement,\n",
    "                \"chunk_metadata\": chunks[i].metadata,\n",
    "                \"chunk_id\": chunk_id\n",
    "            })\n",
    "\n",
    "    print(f\"Extracted a total of {len(statements_with_chunk_info)} statements from all chunks.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Extract Temporal Ranges and Triplets for all statements in parallel\n",
    "    # =========================================================================\n",
    "    date_chain_inputs = [\n",
    "        {\n",
    "            \"statement\": item[\"raw_statement\"].statement,\n",
    "            \"statement_type\": item[\"raw_statement\"].statement_type.value,\n",
    "            \"temporal_type\": item[\"raw_statement\"].temporal_type.value,\n",
    "            \"publication_date\": item[\"chunk_metadata\"][\"date\"].isoformat(),\n",
    "            \"quarter\": item[\"chunk_metadata\"][\"quarter\"]\n",
    "        }\n",
    "        for item in statements_with_chunk_info\n",
    "    ]\n",
    "    \n",
    "    triplet_chain_inputs = [\n",
    "        {\n",
    "            \"statement\": item[\"raw_statement\"].statement,\n",
    "            \"predicate_instructions\": predicate_instructions_text\n",
    "        }\n",
    "        for item in statements_with_chunk_info\n",
    "    ]\n",
    "\n",
    "    # Run both batch extractions concurrently\n",
    "    all_raw_temporal_ranges = date_extraction_chain.batch(date_chain_inputs)\n",
    "    all_raw_extractions = triplet_extraction_chain.batch(triplet_chain_inputs)\n",
    "    \n",
    "    print(\"Completed batch extraction for dates and triplets.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 3: Assemble the final TemporalEvents, Entities, and Triplets\n",
    "    # =========================================================================\n",
    "    final_events = []\n",
    "    final_entities = []\n",
    "    final_triplets = []\n",
    "\n",
    "    for i in range(len(statements_with_chunk_info)):\n",
    "        item = statements_with_chunk_info[i]\n",
    "        raw_statement = item[\"raw_statement\"]\n",
    "        \n",
    "        # Parse the raw temporal range into our final model\n",
    "        temporal_range = TemporalValidityRange.model_validate(all_raw_temporal_ranges[i].model_dump())\n",
    "        \n",
    "        # Assemble entities and triplets for this statement\n",
    "        raw_extraction = all_raw_extractions[i]\n",
    "        idx_to_entity_map = {}\n",
    "        statement_entities = []\n",
    "        for raw_entity in raw_extraction.entities:\n",
    "            entity = Entity(name=raw_entity.name, type=raw_entity.type, description=raw_entity.description)\n",
    "            idx_to_entity_map[raw_entity.entity_idx] = entity\n",
    "            statement_entities.append(entity)\n",
    "        \n",
    "        statement_triplets = []\n",
    "        for raw_triplet in raw_extraction.triplets:\n",
    "            if raw_triplet.subject_id in idx_to_entity_map and raw_triplet.object_id in idx_to_entity_map:\n",
    "                subject_entity = idx_to_entity_map[raw_triplet.subject_id]\n",
    "                object_entity = idx_to_entity_map[raw_triplet.object_id]\n",
    "                triplet = Triplet(\n",
    "                    subject_name=subject_entity.name, subject_id=subject_entity.id,\n",
    "                    predicate=raw_triplet.predicate,\n",
    "                    object_name=object_entity.name, object_id=object_entity.id,\n",
    "                    value=raw_triplet.value\n",
    "                )\n",
    "                statement_triplets.append(triplet)\n",
    "        \n",
    "        # Create the final TemporalEvent\n",
    "        event = TemporalEvent(\n",
    "            chunk_id=item[\"chunk_id\"],\n",
    "            statement=raw_statement.statement,\n",
    "            statement_type=raw_statement.statement_type,\n",
    "            temporal_type=raw_statement.temporal_type,\n",
    "            valid_at=temporal_range.valid_at,\n",
    "            invalid_at=temporal_range.invalid_at,\n",
    "            triplets=[t.id for t in statement_triplets]\n",
    "        )\n",
    "        \n",
    "        final_events.append(event)\n",
    "        final_entities.extend(statement_entities)\n",
    "        final_triplets.extend(statement_triplets)\n",
    "        \n",
    "    print(f\"Assembled {len(final_events)} TemporalEvents, {len(final_entities)} Entities, and {len(final_triplets)} Triplets.\")\n",
    "    \n",
    "    # Update the state with the results\n",
    "    return {\n",
    "        \"chunks\": chunks,\n",
    "        \"temporal_events\": final_events,\n",
    "        \"entities\": final_entities,\n",
    "        \"triplets\": final_triplets,\n",
    "    }\n",
    "\n",
    "print(\"Extraction node function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-automation-workflow",
   "metadata": {},
   "source": [
    "Now, we can define our workflow. For now, it will be a simple graph with just one step: `extract_events`. We will add more steps for cleaning the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the single node we will use\n",
    "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
    "\n",
    "# Set the entrypoint\n",
    "workflow.set_entry_point(\"extract_events\")\n",
    "\n",
    "# Add the single edge in our simple graph\n",
    "workflow.add_edge(\"extract_events\", END)\n",
    "\n",
    "# Compile the graph into a runnable object\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-automation-testing",
   "metadata": {},
   "source": [
    "Let’s run our new automated pipeline on all the chunks we created from our single sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to our graph is a dictionary that matches the GraphState structure.\n",
    "# We only need to provide the initial 'chunks'.\n",
    "graph_input = {\"chunks\": chunked_documents_lc}\n",
    "\n",
    "# Invoke the graph. This will run our entire extraction pipeline.\n",
    "final_state = app.invoke(graph_input)\n",
    "\n",
    "print(\"\\n--- Graph execution complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-automation-analysis",
   "metadata": {},
   "source": [
    "With a single `.invoke()` call, our LangGraph app processed all 19 chunks from our document, ran hundreds of parallel LLM calls, and assembled all the results into a clean, final state.\n",
    "\n",
    "Let’s inspect the output to see the scale of what we’ve accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the number of objects created\n",
    "num_events = len(final_state['temporal_events'])\n",
    "num_entities = len(final_state['entities'])\n",
    "num_triplets = len(final_state['triplets'])\n",
    "\n",
    "print(f\"Total TemporalEvents created: {num_events}\")\n",
    "print(f\"Total Entities created: {num_entities}\")\n",
    "print(f\"Total Triplets created: {num_triplets}\")\n",
    "\n",
    "print(\"\\n--- Sample TemporalEvent from the final state ---\")\n",
    "# Print a sample event to see the fully assembled object\n",
    "sample_event = final_state['temporal_events'][5] # Taking the 6th event as a sample\n",
    "print(sample_event.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-intro",
   "metadata": {},
   "source": [
    "## Cleaning Our Data with Entity Resolution\n",
    "Our automated pipeline is now extracting a good amount of information. However, if you look closely at the entities, you might notice a problem.\n",
    "\n",
    "> An LLM might extract “AMD”, “Advanced Micro Devices”, and “Advanced Micro Devices, Inc” from different parts of the text.\n",
    "\n",
    "To a human, these are obviously the same company, but to a computer, they are just different strings.\n",
    "\n",
    "![Entity Resolution](https://miro.medium.com/v2/resize:fit:2000/1*A7-1cnwMTaYQQSkEFqZEcA.png)\n",
    "\n",
    "This is a critical problem. If we don’t fix it, our knowledge graph will be messy and unreliable. Queries about “AMD” would miss facts connected to “Advanced Micro Devices”.\n",
    "\n",
    "![Entity Resolution](https://miro.medium.com/v2/resize:fit:1400/1*-ZO18JPefxZcUNBWZAqlww.png)\n",
    "\n",
    "This is known as **Entity Resolution**. The goal is to identify all the different names (or “mentions”) that refer to the same real-world entity and merge them under a single, authoritative, “canonical” ID.\n",
    "\n",
    "To solve this, we will add a new quality control station to our LangGraph assembly line. This node will:\n",
    "\n",
    "1.  Cluster entities with similar names using fuzzy string matching.\n",
    "2.  Assign a single, canonical ID to all entities in a cluster.\n",
    "3.  Update our triplets to use these new, clean IDs.\n",
    "\n",
    "To keep track of our canonical entities, we need a place to store them. For this tutorial, we’ll create a simple, in-memory database using Python’s built-in `sqlite3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# This function will create our DB and the necessary 'entities' table.\n",
    "def setup_in_memory_db():\n",
    "    \"\"\"Sets up an in-memory SQLite database and creates the entities table.\"\"\"\n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS entities (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        type TEXT,\n",
    "        description TEXT,\n",
    "        is_canonical INTEGER DEFAULT 1\n",
    "    )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Create our database connection\n",
    "db_conn = setup_in_memory_db()\n",
    "print(\"In-memory SQLite database for canonical entities has been set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-node",
   "metadata": {},
   "source": [
    "Now we’ll create the function for our new LangGraph node. This function will contain the logic for finding and merging duplicate entities. For the fuzzy string matching, we’ll use a handy library called `rapidfuzz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from rapidfuzz import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "def resolve_entities_in_state(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    A LangGraph node to perform entity resolution on the extracted entities.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Entering Node: resolve_entities_in_state ---\")\n",
    "    entities = state[\"entities\"]\n",
    "    triplets = state[\"triplets\"]\n",
    "    \n",
    "    # In a real app, this would be a DB call. We'll simulate it.\n",
    "    # For the first run, the DB is empty.\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(\"SELECT id, name FROM entities WHERE is_canonical = 1\")\n",
    "    global_canonicals = {row[1]: uuid.UUID(row[0]) for row in cursor.fetchall()}\n",
    "    \n",
    "    print(f\"Starting resolution with {len(entities)} entities. Found {len(global_canonicals)} canonicals in DB.\")\n",
    "\n",
    "    # Group entities by type (e.g., 'Person', 'Organization')\n",
    "    type_groups = defaultdict(list)\n",
    "    for entity in entities:\n",
    "        type_groups[entity.type].append(entity)\n",
    "\n",
    "    # --- Fuzzy Matching Logic ---\n",
    "    def clean_name(name: str) -> str:\n",
    "        return name.lower().strip().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    resolved_id_map = {} # Maps an old entity ID to its new canonical ID\n",
    "    newly_created_canonicals = {} # Tracks new canonicals created in this run\n",
    "\n",
    "    for entity_type, group in type_groups.items():\n",
    "        if not group:\n",
    "            continue\n",
    "        \n",
    "        # Cluster entities in the group by fuzzy name matching\n",
    "        clusters = []\n",
    "        used_indices = set()\n",
    "        for i in range(len(group)):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            current_cluster = [group[i]]\n",
    "            used_indices.add(i)\n",
    "            for j in range(i + 1, len(group)):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "                score = fuzz.partial_ratio(clean_name(group[i].name), clean_name(group[j].name))\n",
    "                if score >= 80.0: # Threshold from the notebook\n",
    "                    current_cluster.append(group[j])\n",
    "                    used_indices.add(j)\n",
    "            clusters.append(current_cluster)\n",
    "\n",
    "        # For each cluster, find the best canonical representation\n",
    "        for cluster in clusters:\n",
    "            # Find the \"medoid\" - the entity name most similar to all others in the cluster\n",
    "            scores = {e.name: 0 for e in cluster}\n",
    "            for i in range(len(cluster)):\n",
    "                for j in range(i + 1, len(cluster)):\n",
    "                    score = fuzz.ratio(clean_name(cluster[i].name), clean_name(cluster[j].name))\n",
    "                    scores[cluster[i].name] += score\n",
    "                    scores[cluster[j].name] += score\n",
    "            \n",
    "            medoid_entity = max(cluster, key=lambda e: scores[e.name])\n",
    "            canonical_name = medoid_entity.name\n",
    "            \n",
    "            # Check if this canonical name already exists globally or was just created\n",
    "            if canonical_name in global_canonicals:\n",
    "                canonical_id = global_canonicals[canonical_name]\n",
    "            elif canonical_name in newly_created_canonicals:\n",
    "                canonical_id = newly_created_canonicals[canonical_name].id\n",
    "            else:\n",
    "                # Create a new canonical entity\n",
    "                canonical_id = medoid_entity.id\n",
    "                newly_created_canonicals[canonical_name] = medoid_entity\n",
    "            \n",
    "            # Map all entities in this cluster to the canonical ID\n",
    "            for entity in cluster:\n",
    "                entity.resolved_id = canonical_id\n",
    "                resolved_id_map[entity.id] = canonical_id\n",
    "\n",
    "    # --- Update Triplets with Resolved IDs ---\n",
    "    for triplet in triplets:\n",
    "        if triplet.subject_id in resolved_id_map:\n",
    "            triplet.subject_id = resolved_id_map[triplet.subject_id]\n",
    "        if triplet.object_id in resolved_id_map:\n",
    "            triplet.object_id = resolved_id_map[triplet.object_id]\n",
    "\n",
    "    # --- Persist new canonicals to our simulated DB ---\n",
    "    if newly_created_canonicals:\n",
    "        print(f\"Adding {len(newly_created_canonicals)} new canonical entities to the DB.\")\n",
    "        new_canonical_data = [\n",
    "            (str(e.id), e.name, e.type, e.description, 1)\n",
    "            for e in newly_created_canonicals.values()\n",
    "        ]\n",
    "        cursor.executemany(\"INSERT INTO entities (id, name, type, description, is_canonical) VALUES (?, ?, ?, ?, ?)\", new_canonical_data)\n",
    "        db_conn.commit()\n",
    "\n",
    "    print(\"Entity resolution complete.\")\n",
    "    # The state's `entities` and `triplets` lists are updated by reference.\n",
    "    return state\n",
    "\n",
    "print(\"Entity resolution node function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-workflow",
   "metadata": {},
   "source": [
    "Now, let’s add this new `resolve_entities` node to our LangGraph workflow. The new, improved flow will be: `Start -> Extract -> Resolve -> End`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the graph to include the new node\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add our two nodes\n",
    "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
    "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
    "\n",
    "# The flow is now: Start -> extract -> resolve -> End\n",
    "workflow.set_entry_point(\"extract_events\")\n",
    "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
    "workflow.add_edge(\"resolve_entities\", END)\n",
    "\n",
    "# Compile the new workflow\n",
    "app_with_resolution = workflow.compile()\n",
    "print(\"LangGraph workflow with entity resolution compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-testing",
   "metadata": {},
   "source": [
    "Let’s run our new two-step pipeline on the same set of chunks and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same input as before\n",
    "graph_input = {\"chunks\": chunked_documents_lc}\n",
    "\n",
    "# Invoke the new graph\n",
    "final_state_with_resolution = app_with_resolution.invoke(graph_input)\n",
    "print(\"\\n--- Graph execution with resolution complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entity-resolution-analysis",
   "metadata": {},
   "source": [
    "The graph ran successfully! It first extracted everything, and then the new `resolve_entities` node kicked in, found duplicate entities, and added the new canonical versions to our database.\n",
    "\n",
    "Let’s inspect the output to see the difference. We can check an entity to see if its `resolved_id` has been set, and check a triplet to confirm it's using the new, clean IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db281266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find an entity and see its resolved_id\n",
    "sample_resolved_entity = next((e for e in final_state_with_resolution['entities'] if e.resolved_id is not None), None)\n",
    "\n",
    "if sample_resolved_entity:\n",
    "    print(\"\\n--- Sample of a Resolved Entity ---\")\n",
    "    print(sample_resolved_entity.model_dump_json(indent=2))\n",
    "else:\n",
    "    print(\"\\nNo sample resolved entity found (this might happen with a small data sample).\")\n",
    "    \n",
    "# Let's also check a triplet to see its updated IDs\n",
    "sample_resolved_triplet = final_state_with_resolution['triplets'][0]\n",
    "print(\"\\n--- Sample Triplet with Resolved IDs ---\")\n",
    "print(sample_resolved_triplet.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-intro",
   "metadata": {},
   "source": [
    "## Making Our Knowledge Dynamic with an Invalidation Agent\n",
    "Our data is now chunked, extracted, structured, and cleaned. But we still haven’t addressed the core challenge of *temporal* data: facts change over time.\n",
    "\n",
    "Imagine our knowledge base contains the fact: `(John Smith) --[HOLDS_ROLE]--> (CFO)`. This is a `DYNAMIC` fact, meaning it's true for a period of time. What happens when our agent reads a new document that says, *\"Jane Doe was appointed CFO on January 1st, 2024\"*?\n",
    "\n",
    "The first fact is now outdated. A simple knowledge base wouldn’t know this, but a *temporal* one must. This is the job of the **Invalidation Agent**: to act as a referee, find these contradictions, and update the old facts to mark them as expired.\n",
    "\n",
    "![Dynamic Knowledge Base](https://miro.medium.com/v2/resize:fit:2000/1*yukaF_FP2b9upcTRS5GwGA.png)\n",
    "\n",
    "To do this, we will add a final node to our LangGraph pipeline. This node will:\n",
    "\n",
    "1.  Generate embeddings for all our new statements to understand their semantic meaning.\n",
    "2.  Compare new `DYNAMIC` facts against existing facts in our database.\n",
    "3.  Use an LLM to make a final judgment on whether a new fact invalidates an old one.\n",
    "4.  If a fact is invalidated, it will update its `invalid_at` timestamp.\n",
    "\n",
    "First, we need to prepare our environment for the invalidation logic. This involves adding the `events` and `triplets` tables to our in-memory database. This simulates a real, persistent knowledge base that the agent can check against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's add the necessary tables to our in-memory DB\n",
    "cursor = db_conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS events (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    chunk_id TEXT,\n",
    "    statement TEXT,\n",
    "    statement_type TEXT,\n",
    "    temporal_type TEXT,\n",
    "    valid_at TEXT,\n",
    "    invalid_at TEXT,\n",
    "    embedding BLOB\n",
    ")\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS triplets (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    event_id TEXT,\n",
    "    subject_id TEXT,\n",
    "    predicate TEXT\n",
    ")\n",
    "\"\"\")\n",
    "db_conn.commit()\n",
    "print(\"Events and triplets tables added to the in-memory database.\")\n",
    "\n",
    "PREDICATE_GROUPS: list[list[str]] = [\n",
    "    [\"IS_A\", \"HAS_A\", \"LOCATED_IN\", \"HOLDS_ROLE\", \"PART_OF\"],\n",
    "    [\"PRODUCES\", \"SELLS\", \"SUPPLIES\", \"DISCONTINUED\", \"SECURED\"],\n",
    "    [\"LAUNCHED\", \"DEVELOPED\", \"ADOPTED_BY\", \"INVESTS_IN\", \"COLLABORATES_WITH\"],\n",
    "    [\"HAS_REVENUE\", \"INCREASED\", \"DECREASED\", \"RESULTED_IN\", \"TARGETS\"],\n",
    "]\n",
    "print(\"Predicate groups defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-prompting",
   "metadata": {},
   "source": [
    "Next, we’ll create a new prompt and chain. This chain is very simple: it will show the LLM two potentially conflicting events and ask for a simple “True” or “False” decision on whether one invalidates the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The invalidation prompt from the notebook. It asks for a simple True/False decision.\n",
    "event_invalidation_prompt_template = \"\"\"\n",
    "Task: Analyze the primary event against the secondary event and determine if the primary event is invalidated by the secondary event.\n",
    "Return \"True\" if the primary event is invalidated, otherwise return \"False\".\n",
    "\n",
    "Invalidation Guidelines:\n",
    "1. An event can only be invalidated if it is DYNAMIC and its `invalid_at` is currently null.\n",
    "2. A STATIC event (e.g., \"X was hired on date Y\") can invalidate a DYNAMIC event (e.g., \"Z is the current employee\").\n",
    "3. Invalidation must be a direct contradiction. For example, \"Lisa Su is CEO\" is contradicted by \"Someone else is CEO\".\n",
    "4. The invalidating event (secondary) must occur at or after the start of the primary event.\n",
    "\n",
    "---\n",
    "Primary Event (the one that might be invalidated):\n",
    "- Statement: {primary_statement}\n",
    "- Type: {primary_temporal_type}\n",
    "- Valid From: {primary_valid_at}\n",
    "- Valid To: {primary_invalid_at}\n",
    "\n",
    "Secondary Event (the new fact that might cause invalidation):\n",
    "- Statement: {secondary_statement}\n",
    "- Type: {secondary_temporal_type}\n",
    "- Valid From: {secondary_valid_at}\n",
    "---\n",
    "\n",
    "Is the primary event invalidated by the secondary event? Answer with only \"True\" or \"False\".\n",
    "\"\"\"\n",
    "\n",
    "invalidation_prompt = ChatPromptTemplate.from_template(event_invalidation_prompt_template)\n",
    "\n",
    "# This chain will output a simple string: \"True\" or \"False\".\n",
    "invalidation_chain = invalidation_prompt | llm\n",
    "\n",
    "print(\"Invalidation prompt and chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-node",
   "metadata": {},
   "source": [
    "Now we’ll write the function for our new LangGraph node. This is the most complex function in our pipeline, but it follows a clear logic: find potentially related facts using embeddings, and then ask the LLM to make the final call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def invalidate_events_in_state(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    A LangGraph node to perform temporal invalidation on dynamic facts.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Entering Node: invalidate_events_in_state ---\")\n",
    "    temporal_events = state[\"temporal_events\"]\n",
    "    triplets = state[\"triplets\"]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Add embeddings to all TemporalEvents for similarity comparison.\n",
    "    # =========================================================================\n",
    "    statements_to_embed = [event.statement for event in temporal_events]\n",
    "    event_embeddings = embeddings.embed_documents(statements_to_embed)\n",
    "    for i, event in enumerate(temporal_events):\n",
    "        event.embedding = event_embeddings[i]\n",
    "    print(f\"Generated embeddings for {len(temporal_events)} events.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Simulate a persistent DB by inserting the current batch.\n",
    "    # In a real pipeline, this node would only operate on *new* events\n",
    "    # and compare them against a *pre-existing, persistent* database.\n",
    "    # =========================================================================\n",
    "    cursor = db_conn.cursor()\n",
    "    # Clear previous data for this demo run\n",
    "    cursor.execute(\"DELETE FROM events\")\n",
    "    cursor.execute(\"DELETE FROM triplets\")\n",
    "    \n",
    "    events_data = [\n",
    "        (str(e.id), str(e.chunk_id), e.statement, e.statement_type.value, e.temporal_type.value, \n",
    "         e.valid_at.isoformat() if e.valid_at else None, \n",
    "         e.invalid_at.isoformat() if e.invalid_at else None, \n",
    "         pickle.dumps(e.embedding)) \n",
    "        for e in temporal_events\n",
    "    ]\n",
    "    triplets_data = [\n",
    "        (str(t.id), str(event.id), str(t.subject_id), t.predicate.value)\n",
    "        for event in temporal_events for t_id in event.triplets\n",
    "        for t in triplets if str(t.id) == str(t_id)\n",
    "    ]\n",
    "    cursor.executemany(\"INSERT INTO events VALUES (?, ?, ?, ?, ?, ?, ?, ?)\", events_data)\n",
    "    cursor.executemany(\"INSERT INTO triplets VALUES (?, ?, ?, ?)\", triplets_data)\n",
    "    db_conn.commit()\n",
    "    print(f\"Persisted {len(events_data)} events and {len(triplets_data)} triplets to the in-memory DB.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 3: Find and process invalidation candidates.\n",
    "    # =========================================================================\n",
    "    # For this demo, we will check every event against every other event.\n",
    "    # This is an N^2 operation and would be optimized in production.\n",
    "    events_to_update = {} # To store updates: {event_id_to_invalidate: new_invalid_at_date}\n",
    "\n",
    "    for i, primary_event in enumerate(tqdm(temporal_events, desc=\"Checking for invalidations\")):\n",
    "        # We only try to invalidate DYNAMIC facts that are not already invalidated.\n",
    "        if primary_event.temporal_type != TemporalType.DYNAMIC or primary_event.invalid_at is not None:\n",
    "            continue\n",
    "            \n",
    "        # Find candidate events that could invalidate the primary event.\n",
    "        # In production, this would be an efficient DB query.\n",
    "        candidates = []\n",
    "        for j, secondary_event in enumerate(temporal_events):\n",
    "            if i == j: continue # An event cannot invalidate itself\n",
    "            \n",
    "            # Simple filtering logic adapted from notebook:\n",
    "            # Must be a FACT and must start after the primary event started.\n",
    "            if secondary_event.statement_type == StatementType.FACT and \\\n",
    "               primary_event.valid_at and secondary_event.valid_at and \\\n",
    "               secondary_event.valid_at >= primary_event.valid_at:\n",
    "                \n",
    "                # Check for semantic similarity\n",
    "                similarity = 1 - cosine(primary_event.embedding, secondary_event.embedding)\n",
    "                if similarity > 0.5: # Similarity threshold\n",
    "                    candidates.append(secondary_event)\n",
    "        \n",
    "        if not candidates:\n",
    "            continue\n",
    "            \n",
    "        # =========================================================================\n",
    "        # Step 4: Use LLM to check candidates.\n",
    "        # =========================================================================\n",
    "        invalidation_inputs = [\n",
    "            {\n",
    "                \"primary_statement\": primary_event.statement,\n",
    "                \"primary_temporal_type\": primary_event.temporal_type.value,\n",
    "                \"primary_valid_at\": primary_event.valid_at.isoformat() if primary_event.valid_at else 'None',\n",
    "                \"primary_invalid_at\": 'None', # We know it's null from the check above\n",
    "                \"secondary_statement\": cand.statement,\n",
    "                \"secondary_temporal_type\": cand.temporal_type.value,\n",
    "                \"secondary_valid_at\": cand.valid_at.isoformat() if cand.valid_at else 'None'\n",
    "            }\n",
    "            for cand in candidates\n",
    "        ]\n",
    "        \n",
    "        invalidation_results = invalidation_chain.batch(invalidation_inputs)\n",
    "        \n",
    "        for k, result in enumerate(invalidation_results):\n",
    "            if result.content.strip().lower() == \"true\":\n",
    "                invalidating_event = candidates[k]\n",
    "                \n",
    "                # If an invalidation is found, mark the primary event for update.\n",
    "                # If multiple events invalidate it, pick the one that happened earliest.\n",
    "                if primary_event.id not in events_to_update or \\\n",
    "                   invalidating_event.valid_at < events_to_update[primary_event.id]['invalid_at']:\n",
    "                    events_to_update[primary_event.id] = {\n",
    "                        \"invalid_at\": invalidating_event.valid_at,\n",
    "                        \"invalidated_by\": invalidating_event.id\n",
    "                    }\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 5: Apply the updates to the events in our state.\n",
    "    # =========================================================================\n",
    "    if events_to_update:\n",
    "        print(f\"Found {len(events_to_update)} invalidations to apply.\")\n",
    "        for event in temporal_events:\n",
    "            if event.id in events_to_update:\n",
    "                update_info = events_to_update[event.id]\n",
    "                event.invalid_at = update_info[\"invalid_at\"]\n",
    "                event.invalidated_by = update_info[\"invalidated_by\"]\n",
    "                # The pydantic model's validator will auto-set 'expired_at'\n",
    "                event.set_expired_at() \n",
    "    else:\n",
    "        print(\"No invalidations found in this batch.\")\n",
    "        \n",
    "    return state\n",
    "    \n",
    "print(\"Invalidation node function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-workflow",
   "metadata": {},
   "source": [
    "Now, let’s add our final `invalidate_events` node to the LangGraph workflow. The completed ingestion pipeline will be: **Extract -> Resolve -> Invalidate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the graph to include the new node\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add all three of our nodes\n",
    "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
    "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
    "workflow.add_node(\"invalidate_events\", invalidate_events_in_state)\n",
    "\n",
    "\n",
    "# Define the complete pipeline flow\n",
    "workflow.set_entry_point(\"extract_events\")\n",
    "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
    "workflow.add_edge(\"resolve_entities\", \"invalidate_events\")\n",
    "workflow.add_edge(\"invalidate_events\", END)\n",
    "\n",
    "# Compile the final ingestion workflow\n",
    "ingestion_app = workflow.compile()\n",
    "print(\"Final LangGraph ingestion workflow compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-testing",
   "metadata": {},
   "source": [
    "Now, let’s run the full, three-step pipeline on our sample document’s chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1849b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same input as before\n",
    "graph_input = {\"chunks\": chunked_documents_lc}\n",
    "\n",
    "# Invoke the final graph\n",
    "final_ingested_state = ingestion_app.invoke(graph_input)\n",
    "print(\"\\n--- Full graph execution with invalidation complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalidation-agent-analysis",
   "metadata": {},
   "source": [
    "The pipeline ran successfully and even found an invalidation to apply! Let’s find that specific event to see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print an invalidated event from the final state\n",
    "invalidated_event = next((e for e in final_ingested_state['temporal_events'] if e.invalidated_by is not None), None)\n",
    "\n",
    "if invalidated_event:\n",
    "    print(\"\\n--- Sample of an Invalidated Event ---\")\n",
    "    print(invalidated_event.model_dump_json(indent=2))\n",
    "    \n",
    "    # Let's also find the event that caused the invalidation\n",
    "    invalidating_event_id = invalidated_event.invalidated_by\n",
    "    invalidating_event = next((e for e in final_ingested_state['temporal_events'] if e.id == invalidating_event_id), None)\n",
    "    \n",
    "    if invalidating_event:\n",
    "        print(\"\\n--- Was Invalidated By this Event ---\")\n",
    "        print(invalidating_event.model_dump_json(indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo invalidated events were found in this run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg-assembly-intro",
   "metadata": {},
   "source": [
    "## Assembling the Temporal Knowledge Graph\n",
    "\n",
    "So far, we have successfully takes raw, messy transcripts and transforms them into a clean, structured, and temporally-aware collection of facts.\n",
    "\n",
    "We have successfully automated:\n",
    "1.  **Extraction**: Pulling out statements, dates, and triplets.\n",
    "2.  **Resolution**: Cleaning up duplicate entities.\n",
    "3.  **Invalidation**: Updating facts that are no longer true.\n",
    "\n",
    "Now, it’s time to take this final, high-quality data and build our knowledge graph. This graph will be the “brain” that our retrieval agent will query to answer user questions.\n",
    "\n",
    "![Assembling Temporal KG](https://miro.medium.com/v2/resize:fit:4800/1*OTFWkCaxejA_Cko2NabjRw.png)\n",
    "\n",
    "A graph is the perfect structure for this kind of data because it’s all about connections.\n",
    "\n",
    "*   **Entities** (like “AMD” or “Lisa Su”) become the **nodes** (the dots).\n",
    "*   **Triplets** (the relationships) become the **edges** (the lines connecting the dots).\n",
    "\n",
    "This structure allows us to easily traverse from one fact to another, which is exactly what a smart retrieval agent needs to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a886848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import uuid\n",
    "\n",
    "def build_graph_from_state(state: GraphState) -> nx.MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Builds a NetworkX graph from the final state of our ingestion pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Building Knowledge Graph from final state ---\")\n",
    "    \n",
    "    entities = state[\"entities\"]\n",
    "    triplets = state[\"triplets\"]\n",
    "    temporal_events = state[\"temporal_events\"]\n",
    "    \n",
    "    # Create a mapping from event_id to the event object for quick lookups\n",
    "    event_map = {event.id: event for event in temporal_events}\n",
    "    \n",
    "    # Create a mapping from entity_id to the entity object\n",
    "    entity_map = {entity.id: entity for entity in entities}\n",
    "    \n",
    "    graph = nx.MultiDiGraph()\n",
    "    \n",
    "    # 1. Add nodes to the graph\n",
    "    # We use the canonical (resolved) ID for the node name in the graph.\n",
    "    # We store the entity's own name and other details as node attributes.\n",
    "    canonical_entities = {}\n",
    "    for entity in entities:\n",
    "        # If an entity has a resolved_id, it points to a canonical entity.\n",
    "        # Otherwise, it is its own canonical entity.\n",
    "        canonical_id = entity.resolved_id if entity.resolved_id else entity.id\n",
    "        \n",
    "        if canonical_id not in canonical_entities:\n",
    "             # Find the entity that represents this canonical ID\n",
    "            canonical_entity_obj = entity_map.get(canonical_id, entity)\n",
    "            canonical_entities[canonical_id] = canonical_entity_obj\n",
    "            \n",
    "            graph.add_node(\n",
    "                str(canonical_id), # Use string representation of UUID for node names\n",
    "                name=canonical_entity_obj.name,\n",
    "                type=canonical_entity_obj.type,\n",
    "                description=canonical_entity_obj.description\n",
    "            )\n",
    "            \n",
    "    print(f\"Added {graph.number_of_nodes()} canonical entity nodes to the graph.\")\n",
    "\n",
    "    # 2. Add edges to the graph from the triplets\n",
    "    edges_added = 0\n",
    "    for triplet in triplets:\n",
    "        event = event_map.get(uuid.UUID(str(triplet.id))) # Find the event associated with this triplet's original ID\n",
    "        # A triplet might not have a direct event if it was part of a larger statement. \n",
    "        # We need to find the event that contains this triplet's ID.\n",
    "        parent_event = None\n",
    "        for ev in temporal_events:\n",
    "            if triplet.id in ev.triplets:\n",
    "                parent_event = ev\n",
    "                break\n",
    "        \n",
    "        if not parent_event:\n",
    "            continue\n",
    "            \n",
    "        # Get the canonical IDs for the subject and object\n",
    "        subject_canonical_id = str(triplet.subject_id)\n",
    "        object_canonical_id = str(triplet.object_id)\n",
    "        \n",
    "        # Ensure both nodes exist in the graph before adding an edge\n",
    "        if graph.has_node(subject_canonical_id) and graph.has_node(object_canonical_id):\n",
    "            # The edge attributes will contain all the rich temporal and contextual info\n",
    "            edge_attrs = {\n",
    "                \"predicate\": triplet.predicate.value,\n",
    "                \"value\": triplet.value,\n",
    "                \"statement\": parent_event.statement,\n",
    "                \"valid_at\": parent_event.valid_at,\n",
    "                \"invalid_at\": parent_event.invalid_at,\n",
    "                \"statement_type\": parent_event.statement_type.value,\n",
    "                \"temporal_type\": parent_event.temporal_type.value,\n",
    "                \"event_id\": str(parent_event.id)\n",
    "            }\n",
    "            \n",
    "            graph.add_edge(\n",
    "                subject_canonical_id,\n",
    "                object_canonical_id,\n",
    "                key=triplet.predicate.value, # The key helps distinguish multiple edges\n",
    "                **edge_attrs\n",
    "            )\n",
    "            edges_added += 1\n",
    "\n",
    "    print(f\"Added {edges_added} edges (relationships) to the graph.\")\n",
    "    return graph\n",
    "\n",
    "# Let's build the graph from the state we got from our LangGraph app\n",
    "knowledge_graph = build_graph_from_state(final_ingested_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg-assembly-analysis",
   "metadata": {},
   "source": [
    "Now that we have our `knowledge_graph` object, let's explore it to see what we've built. We can inspect a specific node, like \"AMD\", to see its properties and relationships.\n",
    "\n",
    "To get a better visual sense of our graph, let’s plot a small subgraph containing the most important, highly-connected entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd91bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Graph has {knowledge_graph.number_of_nodes()} nodes and {knowledge_graph.number_of_edges()} edges.\")\n",
    "\n",
    "# --- Inspect a sample node ---\n",
    "# Let's find the node for \"AMD\"\n",
    "amd_node_id = None\n",
    "for node, data in knowledge_graph.nodes(data=True):\n",
    "    if data.get('name', '').lower() == 'amd':\n",
    "        amd_node_id = node\n",
    "        break\n",
    "\n",
    "if amd_node_id:\n",
    "    print(\"\\n--- Inspecting the 'AMD' node ---\")\n",
    "    print(f\"Node ID: {amd_node_id}\")\n",
    "    print(f\"Attributes: {knowledge_graph.nodes[amd_node_id]}\")\n",
    "    \n",
    "    # --- Inspect edges connected to the \"AMD\" node ---\n",
    "    print(\"\\n--- Sample Outgoing Edges from 'AMD' ---\")\n",
    "    for i, (u, v, data) in enumerate(knowledge_graph.out_edges(amd_node_id, data=True)):\n",
    "        if i >= 3: break # Show first 3\n",
    "        object_name = knowledge_graph.nodes[v]['name']\n",
    "        print(f\"Edge {i+1}: AMD --[{data['predicate']}]--> {object_name}\")\n",
    "        print(f\"  Statement: '{data['statement'][:80]}...'\")\n",
    "        print(f\"  Valid From: {data['valid_at']}\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find a node for 'AMD'.\")\n",
    "\n",
    "# --- Visualize a subgraph of the most connected nodes ---\n",
    "# This helps to see the main entities and their relationships.\n",
    "degrees = dict(knowledge_graph.degree())\n",
    "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:15]\n",
    "\n",
    "subgraph = knowledge_graph.subgraph(top_nodes)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(subgraph, k=0.5, iterations=50)\n",
    "\n",
    "# Get node labels from their 'name' attribute\n",
    "labels = {node: data['name'] for node, data in subgraph.nodes(data=True)}\n",
    "\n",
    "nx.draw(subgraph, pos, labels=labels, with_labels=True, node_color='lightblue', \n",
    "        node_size=2000, edge_color='gray', font_size=10, font_weight='bold')\n",
    "\n",
    "plt.title(\"Subgraph of Top 15 Most Connected Entities\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-agent-intro",
   "metadata": {},
   "source": [
    "## Building and Testing A Multi-Step Retrieval Agent\n",
    "\n",
    "We’ve successfully built our “smart database” a rich, clean, and temporally-aware knowledge graph. Now comes the payoff, building an intelligent agent that can have a conversation with this graph to answer complex questions.\n",
    "\n",
    "> ### Why Single-Step RAG Isn’t Enough\n",
    "\n",
    "A simple RAG system might find one relevant fact and use it to answer a question. But what if the answer requires connecting multiple pieces of information?\n",
    "\n",
    "Consider a question like:\n",
    "> *How did AMD’s focus on data centers evolve between 2016 and 2017?*\n",
    "\n",
    "![Multi Agent System](https://miro.medium.com/v2/resize:fit:2000/1*e4vI9o0eeMq7TX4rW9t_qQ.png)\n",
    "\n",
    "A simple retrieval system can’t answer this. It requires a **multi-step** process:\n",
    "\n",
    "1.  Find facts about AMD and data centers in 2016.\n",
    "2.  Find facts about AMD and data centers in 2017.\n",
    "3.  Compare the results from both years.\n",
    "4.  Synthesize a final summary.\n",
    "\n",
    "This is what our multi-step retrieval agent will do. We will build it using LangGraph, and it will have three key components: a Planner, a set of Tools, and an Orchestrator.\n",
    "\n",
    "Before our main agent starts working, we want it to think and create a high-level plan. This makes the agent more reliable and focused. The planner is a simple LLM chain whose only job is to break down the user’s question into a sequence of actionable steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc50cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It uses a \"persona\" to guide the LLM's output style.\n",
    "initial_planner_system_prompt = (\n",
    "    \"You are an expert financial research assistant. Your task is to create a step-by-step plan \"\n",
    "    \"for answering a user's question by querying a temporal knowledge graph of earnings call transcripts. \"\n",
    "    \"The available tool is `factual_qa`, which can retrieve facts about an entity for a specific topic (predicate) \"\n",
    "    \"within a given date range. Your plan should consist of a series of calls to this tool.\"\n",
    ")\n",
    "\n",
    "initial_planner_user_prompt_template = \"\"\"\n",
    "User Question: \"{user_question}\"\n",
    "\n",
    "Based on this question, create a concise, step-by-step plan. Each step should be a clear action for querying the knowledge graph.\n",
    "\n",
    "Return only the plan under a heading 'Research tasks'.\n",
    "\"\"\"\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", initial_planner_system_prompt),\n",
    "    (\"user\", initial_planner_user_prompt_template),\n",
    "])\n",
    "\n",
    "# We'll use a powerful model for planning, as recommended.\n",
    "planner_chain = planner_prompt | llm\n",
    "\n",
    "# Let's test the planner with a sample question.\n",
    "user_question = \"How did AMD's focus on data centers evolve between 2016 and 2017?\"\n",
    "\n",
    "print(f\"--- Generating plan for question: '{user_question}' ---\")\n",
    "plan_result = planner_chain.invoke({\"user_question\": user_question})\n",
    "initial_plan = plan_result.content\n",
    "\n",
    "print(\"\\n--- Generated Plan ---\")\n",
    "print(initial_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-agent-tooling",
   "metadata": {},
   "source": [
    "Now we build the final agent. This agent will act as an **“orchestrator”**. It will look at the user’s question and the plan, and then intelligently decide which tools to call in a loop until it has enough information to provide a final answer.\n",
    "\n",
    "We will build this orchestrator using LangGraph, which is perfect for managing this kind of cyclical, **“decide-and-act”** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import date\n",
    "import datetime as dt # Use alias to avoid confusion with the date object\n",
    "\n",
    "def _as_datetime(ts) -> dt.datetime | None:\n",
    "    if ts is None:\n",
    "        return None\n",
    "    if isinstance(ts, dt.datetime):\n",
    "        return ts\n",
    "    if isinstance(ts, dt.date):\n",
    "        return dt.datetime.combine(ts, dt.datetime.min.time())\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y-%m-%dT%H:%M:%S\"):\n",
    "        try:\n",
    "            return dt.datetime.strptime(ts, fmt)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "@tool\n",
    "def factual_qa(entity: str, start_date: date, end_date: date, predicate: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the knowledge graph for facts about a specific entity, topic (predicate),\n",
    "    and time range. Returns a formatted string of matching relationships.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- TOOL CALL: factual_qa ---\")\n",
    "    print(f\"  - Entity: {entity}\")\n",
    "    print(f\"  - Date Range: {start_date} to {end_date}\")\n",
    "    print(f\"  - Predicate: {predicate}\")\n",
    "\n",
    "    # Convert input dates to timezone-aware datetimes for comparison\n",
    "    start_dt = _as_datetime(start_date).replace(tzinfo=timezone.utc)\n",
    "    end_dt = _as_datetime(end_date).replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # 1. Find the entity node in the graph\n",
    "    target_node_id = None\n",
    "    for node_id, data in knowledge_graph.nodes(data=True):\n",
    "        if entity.lower() in data.get('name', '').lower():\n",
    "            target_node_id = node_id\n",
    "            break\n",
    "            \n",
    "    if not target_node_id:\n",
    "        return f\"Error: Entity '{entity}' not found in the knowledge graph.\"\n",
    "\n",
    "    # 2. Search for matching edges\n",
    "    matching_edges = []\n",
    "    # Search both outgoing and incoming edges\n",
    "    for u, v, data in list(knowledge_graph.out_edges(target_node_id, data=True)) + \\\n",
    "                      list(knowledge_graph.in_edges(target_node_id, data=True)):\n",
    "        \n",
    "        if predicate.upper() in data.get('predicate', '').upper():\n",
    "            valid_at = data.get('valid_at')\n",
    "            if valid_at:\n",
    "                # Ensure valid_at is a timezone-aware datetime for comparison\n",
    "                if not valid_at.tzinfo:\n",
    "                    valid_at = valid_at.replace(tzinfo=timezone.utc)\n",
    "                \n",
    "                if start_dt <= valid_at <= end_dt:\n",
    "                    subject_name = knowledge_graph.nodes[u]['name']\n",
    "                    object_name = knowledge_graph.nodes[v]['name']\n",
    "                    matching_edges.append(\n",
    "                        f\"Fact: {subject_name} --[{data['predicate']}]--> {object_name} \"\n",
    "                        f\"(Statement: '{data['statement'][:100]}...')\"\n",
    "                    )\n",
    "\n",
    "    if not matching_edges:\n",
    "        return f\"No facts found for entity '{entity}' with predicate '{predicate}' between {start_date} and {end_date}.\"\n",
    "        \n",
    "    return \"\\n\".join(matching_edges)\n",
    "\n",
    "# Let's test the tool directly\n",
    "test_result = factual_qa.invoke({\n",
    "    \"entity\": \"AMD\",\n",
    "    \"predicate\": \"LAUNCHED\",\n",
    "    \"start_date\": dt.date(2016, 1, 1),\n",
    "    \"end_date\": dt.date(2017, 12, 31)\n",
    "})\n",
    "\n",
    "print(\"\\n--- Tool Test Result ---\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd794e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corrected import is for ToolNode.\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "# 1. Set up the tools for the ToolNode\n",
    "tools = [factual_qa]\n",
    "\n",
    "# The AgentState definition remains the same\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "\n",
    "# 2. Define the agent nodes\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    last_message = state['messages'][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    print(\"\\n--- AGENT: Calling model... ---\")\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 3. Wire up the agent graph\n",
    "# =========================================================================\n",
    "# === THIS IS THE LINE TO CHANGE ===\n",
    "# We add .bind(tool_choice=\"any\") to force the model to use a tool if it can.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"any\")\n",
    "# =========================================================================\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", ToolNode(tools))\n",
    "\n",
    "# Define the edges\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "retrieval_agent = workflow.compile()\n",
    "print(\"Retrieval agent graph compiled successfully with tool_choice parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-agent-testing",
   "metadata": {},
   "source": [
    "Let’s run the full process and ask our question. We’ll give the agent the user question *and* the plan we generated earlier to give it the best possible start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block does not need any changes.\n",
    "initial_message = HumanMessage(\n",
    "    content=f\"Here is my question: '{user_question}'\\n\\n\"\n",
    "            f\"Here is the plan to follow:\\n{initial_plan}\"\n",
    ")\n",
    "\n",
    "agent_input = {\"messages\": [initial_message]}\n",
    "\n",
    "print(\"\\n--- Running the full retrieval agent (with tool_choice) ---\")\n",
    "\n",
    "# Stream the agent's execution\n",
    "async for output in retrieval_agent.astream(agent_input):\n",
    "    for key, value in output.items():\n",
    "        if key == \"agent\":\n",
    "            agent_message = value['messages'][-1]\n",
    "            if agent_message.tool_calls:\n",
    "                print(f\"LLM wants to call a tool: {agent_message.tool_calls[0]['name']}\")\n",
    "            else:\n",
    "                print(\"\\n--- AGENT: Final Answer ---\")\n",
    "                print(agent_message.content)\n",
    "        elif key == \"action\":\n",
    "            print(\"--- AGENT: Tool response received. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## What Can We Do Next?\n",
    "We have built a powerful prototype that demonstrates how to create a knowledge base that isn’t just a static library, but a dynamic system that understands how facts evolve over time.\n",
    "\n",
    "Now that our agent works, the next critical question is: “How *well* does it work?” Answering this requires a formal evaluation process. There are three main approaches:\n",
    "\n",
    "*   **Golden Answers (The Gold Standard):** You create a test set of questions and have human experts write the perfect answers. You then compare your agent’s output to these “golden” answers. This is the most accurate method but is slow and expensive.\n",
    "*   **LLM-as-Judge (The Scalable Approach):** You use a powerful LLM (like GPT-4) to act as a “judge.” It scores your agent’s answers for correctness and relevance. This is fast and cheap, making it perfect for rapid testing and iteration.\n",
    "*   **Human Feedback (The Real-World Test):** Once your agent is deployed, you can add simple feedback buttons (like thumbs-up/thumbs-down) to let users rate the answers. This tells you how useful your agent is for real tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a77d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-temporal-logic (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
